# OpenWebText2

This project is part of Eleuther AI's quest to create a massive repository of high quality text data for training language models.

Very briefly, OpenWebText2 is a large filtered dataset of text documents scraped from URL found on Reddit submisisons.

The plug and play version of OpenWebText2 contains:<br/>
17,103,059 documents 
65.86GB uncompressed text

**Acknowledgements**  
Much of this code was written by @researcher2, with inspiration and some straight copying of the scraping code found [here](https://github.com/yet-another-account/openwebtext/). @sdtblck kindly put together the Colab notebook, and performed a chunk of the scraping. @leogao2 provided overall design guidance, lm_dataformat, and performed another chunk of scraping. Thanks to [Colaboratory](https://colab.research.google.com/) for the vms, they helped us with about 10% of our overall scraping.

## Download / Replicate The Dataset / Full Documentation

For further information please visit our [documentation](https://openwebtext2.readthedocs.io/en/latest/).
