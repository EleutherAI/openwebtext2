<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Dataset Replication - OpenWebText2</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="../css/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Dataset Replication";
    var mkdocs_page_input_path = "replication.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> OpenWebText2</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Welcome</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../background/">WebText Background</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Dataset Replication</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#pipeline-overview">Pipeline Overview</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#environment-setup">Environment Setup</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#miniconda-install-for-linux">Miniconda Install For Linux</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#create-and-activate-conda-environment">Create and activate conda environment</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#install-repo-and-requirements">Install Repo and Requirements</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#general-recommendations">General Recommendations</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#stage-1-processing-pushshift-submission-dumps">Stage 1 - Processing PushShift Submission Dumps</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#sqlite-database-setup">Sqlite Database Setup</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#insert-pushshift-submission-data-into-sqlite">Insert PushShift Submission Data Into Sqlite</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#extract-unique-urls-with-reddit-metadata">Extract Unique URLs with Reddit Metadata</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#stage-2-scraping-from-sourced-urls">Stage 2 - Scraping From Sourced URLs</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#stage-3-filtering-scraped-documents-by-minimum-total-reddit-score">Stage 3 - Filtering scraped documents by minimum total Reddit score</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#stage-4-deduplicate-filtered-documents-using-minhashlsh-with-cassandra">Stage 4 - Deduplicate Filtered Documents using MinHashLSH with Cassandra</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#setup-cassandra">Setup Cassandra</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#generate-minhashes-for-every-document">Generate Minhashes For Every Document</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slice-the-minhashes-for-batching">Slice The Minhashes For Batching</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#find-duplicates-using-minhashlsh-with-cassandra">Find Duplicates Using MinHashLSH with Cassandra</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#de-duplicating-using-generated-duplicate-lists">De-Duplicating Using Generated Duplicate Lists</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#stage-5-packaging-the-dataset-releases">Stage 5 - Packaging The Dataset Releases</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#plug-and-play-release">Plug And Play Release</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#raw-scrapes-release">Raw Scrapes Release</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#stage-6-produce-release-stats">Stage 6 - Produce Release Stats</a>
    </li>
    </ul>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">OpenWebText2</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Dataset Replication</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="dataset-replication">Dataset Replication</h1>
<p>This area of the documentation provides instructions for building the full dataset from scratch. If you just want the dataset, please see <a href="/">Welcome</a>.</p>
<p><a href="https://www.reddit.com/r/pushshift/comments/bcxguf/new_to_pushshift_read_this_faq" target="_blank">PushShift</a> provides dumps of all reddit posts and submissions, however they are normally a few months behind. While this would be problematic for certain use cases, we didn't require up to the minute data for training GPTNeo. In the future we may look into getting recent data either by scraping Reddit directly or using one of the existing APIs. </p>
<h2 id="pipeline-overview">Pipeline Overview</h2>
<p>At a high level the pipeline works as follows:</p>
<ol>
<li>Download and process the PushShift submission dumps to extract unique URLs &amp; Metadata.</li>
<li>Scrape the URLs using <a href="https://newspaper.readthedocs.io/en/latest/" target="_blank">Newspaper3k</a>, saving both text and metadata with <a href="https://github.com/leogao2/lm_dataformat" target="_blank">lm_dataformat</a>.</li>
<li>Filter the scraped documents by minimum Reddit score 3.</li>
<li>Perform fuzzy deduplication using <a href="http://ekzhu.com/datasketch/lsh.html" target="_blank">MinHashLSH</a>.</li>
<li>Package up the various dataset releases.</li>
<li>Produce some useful size stats for the releases.</li>
</ol>
<h2 id="environment-setup">Environment Setup</h2>
<p>We tested everything on Ubuntu 18/20 &amp; Windows 10 with miniconda. You could use virtualenv, venv or even the global python environment if you wish.</p>
<h3 id="miniconda-install-for-linux">Miniconda Install For Linux</h3>
<p>Follow the below steps, or read the conda instructions:<br/>
<a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html" target="_blank">https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html</a></p>
<pre><code class="language-bash">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
sha256 Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
</code></pre>
<p>Select yes on the init step.</p>
<p>Restart your shell to refresh the path.</p>
<h3 id="create-and-activate-conda-environment">Create and activate conda environment</h3>
<p>Environments are saved in a central store on the local disk, no need to create folders like with venv.</p>
<pre><code>conda create --name pushshift python=3.8
conda activate pushshift
</code></pre>
<h3 id="install-repo-and-requirements">Install Repo and Requirements</h3>
<pre><code class="language-bash">git clone https://github.com/EleutherAI/openwebtext2.git
cd openwebtext2
pip install -r requirements.txt
</code></pre>
<h3 id="general-recommendations">General Recommendations</h3>
<p>Use the screen command if running a remote terminal, many scripts below take a long time to run and while they often support resuming it's better not to rely on it.</p>
<p>Create a working directory on a drive with at least 500gb of space.</p>
<h2 id="stage-1-processing-pushshift-submission-dumps">Stage 1 - Processing PushShift Submission Dumps</h2>
<p>This stage consists of the following steps:</p>
<ol>
<li>Sqlite database setup.</li>
<li>Download and verify the PushShift submission dumps, extracting and storing urls and metadata
from relevant submissions into the sqlite database. Performed by "pushshift/pushshift_to_sqlite.py".</li>
<li>Query the populated sqlite database and build a list of URLs with metadata for all related submissions.
Performed by "pushshift/generate_urls_from_sqlite.py"</li>
</ol>
<p>All scripts for this stage can be found within the "pushshift" package.</p>
<h3 id="sqlite-database-setup">Sqlite Database Setup</h3>
<p>We use alembic to manage the sqlite database in case you want to add extra indexes or fields easily later.</p>
<p>Inside the cloned repo:</p>
<pre><code class="language-bash">cp alembic.ini.template alembic.ini
</code></pre>
<p>Modify the following line within the alembic.ini file. For example on windows:</p>
<pre><code class="language-python">sqlalchemy.url = sqlite:///e:/Eleuther_AI/openwebtext2/submissions.sqlite
</code></pre>
<p>Or on Linux (notice the extra forward slash before "mnt" to indicate system root):</p>
<pre><code class="language-python">sqlalchemy.url = sqlite:////mnt/data/openwebtext2/submissions.sqlite
</code></pre>
<h3 id="insert-pushshift-submission-data-into-sqlite">Insert PushShift Submission Data Into Sqlite</h3>
<p>This step is performed by <em>pushshift/pushshift_to_sqlite.py</em>.</p>
<table>
<thead>
<tr>
<th align="right">Script Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code>--start_period (-s)</code></td>
<td>Month and Year of first pushshift dump. Default: 6,2005.</td>
</tr>
<tr>
<td align="right"><code>--finish_period (-f)</code></td>
<td>Month and Year of final pushshift dump. Defaults to current month.</td>
</tr>
<tr>
<td align="right"><code>--output_directory (-dir)</code></td>
<td>Will contain the dumps subdirectory created as part of the process.</td>
</tr>
<tr>
<td align="right"><code>--keep_dumps (-kd)</code></td>
<td>If specified the dumps won't be deleted after successful processing.</td>
</tr>
</tbody>
</table>
<p>Notice the database location is not specified here, this is always sourced from the alembic.ini file.</p>
<p>For example on Linux, to download and process all dumps, leaving the downloaded dumps afterwards:</p>
<pre><code class="language-bash">python -m pushshift.pushshift_to_sqlite -dir /mnt/data/openwebtext2 -kd
</code></pre>
<p>Test run on 2006 only, deleting dumps when done:</p>
<pre><code class="language-bash">python -m pushshift.pushshift_to_sqlite -s 1,2006 -f 12,2006 -dir /mnt/data/openwebtext2
</code></pre>
<p>This step uses checkpointing, saving a .dbdone file for each dump once processing is complete. So if you need to stop and come back later you can.</p>
<h3 id="extract-unique-urls-with-reddit-metadata">Extract Unique URLs with Reddit Metadata</h3>
<p>This step is performed by <em>pushshift/generate_urls_from_sqlite.py</em>.</p>
<table>
<thead>
<tr>
<th align="right">Script Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code>--start_period (-s)</code></td>
<td>Month and Year of first URLs. Defaults to None (query all URLs).</td>
</tr>
<tr>
<td align="right"><code>--finish_period (-f)</code></td>
<td>Month and Year of final URLs. Defaults to None (query all URLs).</td>
</tr>
<tr>
<td align="right"><code>--output_directory (-dir)</code></td>
<td>Will contain the urls subdirectory created as part of the process.</td>
</tr>
<tr>
<td align="right"><code>--urls_per_file</code></td>
<td>Maximum number of urls per file. Defaults to 100,000.</td>
</tr>
</tbody>
</table>
<p>Notice the database location is not specified here, this is always sourced from the alembic.ini file.</p>
<p>For example on Linux, to extract all urls </p>
<pre><code class="language-bash">python -m pushshift.generate_urls_from_sqlite -dir /mnt/data/openwebtext2
</code></pre>
<p>Test run on 2006 only:</p>
<pre><code class="language-bash">python -m pushshift.generate_urls_from_sqlite -s 1,2006 -f 12,2006 -dir /mnt/data/openwebtext2
</code></pre>
<h2 id="stage-2-scraping-from-sourced-urls">Stage 2 - Scraping From Sourced URLs</h2>
<p>This stage is performed by <em>scraping/scrape_urls.py</em> and took several weeks compute time. To decrease this you can run on multiple servers by passing out the URL files.</p>
<table>
<thead>
<tr>
<th align="right">Script Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code>--job_directory (-dir)</code></td>
<td>Base directory containing the urls subdirectory and location where the scrapes subdirectory will be created.</td>
</tr>
<tr>
<td align="right"><code>--process_count (-procs)</code></td>
<td>Number of worker processes in the pool. Defaults to 60. Don't go above this on Windows.</td>
</tr>
<tr>
<td align="right"><code>--request_timeout (-timeout)</code></td>
<td>Scraping timeout for each URL. Defaults to 30 seconds.</td>
</tr>
</tbody>
</table>
<p>The script iterates through URL files generated in step 2 above. For each file its hands out the URLs
to a multiprocessing pool for scraping. Once all URLs in the batch are scraped, the successful results are 
archived using a slightly modified version of <a href="https://github.com/leogao2/lm_dataformat" target="_blank">lm_dataformat</a>. For each document (URL), the following metadata fields are saved in the metadata dict offered by lm_dataformat:</p>
<table>
<thead>
<tr>
<th align="right">Meta Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">title</td>
<td>Web Page Title.</td>
</tr>
<tr>
<td align="right">lang</td>
<td>Language detected by Newspaper scraper.</td>
</tr>
<tr>
<td align="right">url</td>
<td>Original URL.</td>
</tr>
<tr>
<td align="right">word_count</td>
<td>Total words outputted by Newspaper.</td>
</tr>
<tr>
<td align="right">elapsed</td>
<td>Scraping time.</td>
</tr>
<tr>
<td align="right">scraper</td>
<td>Always "newspaper".</td>
</tr>
<tr>
<td align="right">domain</td>
<td>Top level domain for the original URL.</td>
</tr>
<tr>
<td align="right">reddit_id</td>
<td>List of submission IDs containing URL - converted from base36.</td>
</tr>
<tr>
<td align="right">subreddit</td>
<td>List of subreddits for the corresponding submissions.</td>
</tr>
<tr>
<td align="right">reddit_score</td>
<td>List of reddit scores for the corresponding submissions.</td>
</tr>
<tr>
<td align="right">reddit_title</td>
<td>List of submissions titles for the corresponding submissions.</td>
</tr>
<tr>
<td align="right">reddit_created_utc</td>
<td>List of submissions created times for the corresponding submissions.</td>
</tr>
</tbody>
</table>
<p>The program will look for URL files within "job_directory/urls". All scrapes will be stored in "job_directory/scrapes"</p>
<p>For example on Linux, this will scrape using 90 processes and 30 second timeout:</p>
<pre><code class="language-bash">python -m scraping.scrape_urls -dir /mnt/data/openwebtext2 -procs 90
</code></pre>
<p>On a dedicated 2012 i7 Linux machine we used between 90 and 120 processes successfully.</p>
<p>We do some limited URL filtering in <em>scraping/filter.py</em>. This is mainly to speed up the process by avoiding timeouts or files that obviously won't contain text.</p>
<p>Once each URL file is scraped, the program saves a ".done" file so you can resume later without rescraping. That file contains a count of successfully scraped URLs if you are interested.</p>
<h2 id="stage-3-filtering-scraped-documents-by-minimum-total-reddit-score">Stage 3 - Filtering scraped documents by minimum total Reddit score</h2>
<p>This stage is performed by <em>cleaning/filter_from_reddit_score.py</em>.</p>
<table>
<thead>
<tr>
<th align="right">Script Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code>--scrape_directory (-dir)</code></td>
<td>Directory containing the scrapes. You could use the overall work directory if you want as we use glob.glob to search recursively.</td>
</tr>
</tbody>
</table>
<p>The script filters all scrape files "scrapes_*.jsonl.zst" by minimum total Reddit score.
Unlike the original WebText we aggregate scores for all submissions containing a given
URL so the bar is slightly lower in some cases, but in others where a URL went negative
in some submission it will balance out.</p>
<p>The filtered scrapes file will have the original name and path of the scrape file with a 
".minscored" extension.</p>
<p>For example on Linux:</p>
<pre><code class="language-bash">python -m cleaning.filter_from_reddit_score -dir /mnt/data/openwebtext2/scrapes
</code></pre>
<h2 id="stage-4-deduplicate-filtered-documents-using-minhashlsh-with-cassandra">Stage 4 - Deduplicate Filtered Documents using MinHashLSH with Cassandra</h2>
<p>There are several sub-stages here:</p>
<ol>
<li>Setup Cassandra</li>
<li>Generate minhashes for every document</li>
<li>Batch up the minhashes for running parallel dedupe</li>
<li>Using MinHashLSH With Cassandra - Generate lists of duplicates</li>
<li>Deduplicating our documents using the lists from step 3.</li>
</ol>
<p>All scripts for this stage can be found within the "cleaning" package.</p>
<h3 id="setup-cassandra">Setup Cassandra</h3>
<p>We used a local Cassandra install, simplifying the setup process. Some good cassandra guides:</p>
<p><a href="https://www.tecmint.com/install-apache-cassandra-on-ubuntu/" target="_blank">Installation Guide For Ubuntu 20</a><br/>
<a href="https://towardsdatascience.com/getting-started-with-apache-cassandra-and-python-81e00ccf17c9" target="_blank">Introduction To Cassandra + Connecting With Python API</a></p>
<p>Summarized Quick Install For Ubuntu 20:</p>
<pre><code class="language-bash">sudo apt install openjdk-8-jdk
sudo apt install apt-transport-https
wget -q -O - https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add -
sudo sh -c 'echo &quot;deb http://www.apache.org/dist/cassandra/debian 311x main&quot; &gt; /etc/apt/sources.list.d/cassandra.list'
sudo apt update
sudo apt install cassandra
sudo systemctl status cassandra
</code></pre>
<p>To test your installation was successful, run the cqlsh CLI:</p>
<pre><code class="language-bash">cqlsh
</code></pre>
<p>Once inside:
<code>describe keyspaces</code></p>
<p>If you want multiple nodes or remote connection you need to set the following in your /etc/cassandra/cassandra.yaml:</p>
<p>seeds: "your_server_external_ip, other nodes in cluster"<br/>
listen_address: your_server_external_ip<br/>
start_rpc: true<br/>
rpc_address: 0.0.0.0 (this will bind to same address as listen_address)</p>
<p>For some reason they recommend not to make this available on the internet despite supporting various forms of authentication. So either use a tunnel or fancy networking to get around this.</p>
<h3 id="generate-minhashes-for-every-document">Generate Minhashes For Every Document</h3>
<p>This step is performed by <em>cleaning/generate_minhashes.py</em> and took about 1.5 days
on a 2012 i7 Linux machine.</p>
<table>
<thead>
<tr>
<th align="right">Script Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code>scrape_directory (-dir)</code></td>
<td>Directory containing the minscored scrapes. You could use the overall work directory if you want as we use glob.glob to search recursively.</td>
</tr>
<tr>
<td align="right"><code>process_count (-procs)</code></td>
<td>Number of worker processes in the pool. Defaults to 4.</td>
</tr>
</tbody>
</table>
<p>This script calculates minhashes for all filtered scrape files found using a recursive
search on "*.minscored".</p>
<p>More explicity, we create a set of 5-grams for each document, and generate document 
level minhashes using 10 hash functions with the excellent datasketch library.</p>
<p>A single file "minhashes.pkl" is created in the scrape directory storing a data
structure in the following format:</p>
<pre><code class="language-python">[(file_name1, [doc0_minhash, doc1_minhash, ...]), (file_name2, [....]), ....]
</code></pre>
<p>For example on Linux:</p>
<pre><code class="language-bash">python -m cleaning.generate_minhashes -dir /mnt/data/openwebtext2/scrapes
</code></pre>
<h3 id="slice-the-minhashes-for-batching">Slice The Minhashes For Batching</h3>
<p>This step is performed by <em>cleaning/minhash_lsh_batching.py</em>.</p>
<table>
<thead>
<tr>
<th align="right">Script Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code>directory (-dir)</code></td>
<td>Directory containing the 'minhashes.pkl' file. Batch files and file name lookup will be saved here.</td>
</tr>
<tr>
<td align="right"><code>number_of_batches (-batches)</code></td>
<td>Approximate number of batches to split minhashes into.</td>
</tr>
</tbody>
</table>
<p>The "directory" must contain a 'minhashes.pkl' file created with <em>cleaning/generate_minhashes.py</em>.</p>
<p>This splits "minhashes.pkl" into approximately the desired number of batches, producing batch files named 'batch0.pkl, batch1.pkl, etc'. They contain the following pickled data structure:</p>
<pre><code class="language-python">[(file_id, [doc0_minhash, doc1_minhash, ...]), ....]
</code></pre>
<p>It also creates a file name lookup named 'file_name_lookup.pkl' containing the following pickled datastructure:</p>
<pre><code class="language-python">[file_name1, file_name2, file_name3, ...]
</code></pre>
<p>For example on Linux with 16 batches:</p>
<pre><code class="language-bash">python -m cleaning.minhash_lsh_batching -dir /mnt/data/openwebtext2/scrapes -batches 16
</code></pre>
<h3 id="find-duplicates-using-minhashlsh-with-cassandra">Find Duplicates Using MinHashLSH with Cassandra</h3>
<p>This step is performed by <em>cleaning/minhash_lsh_dedupe.py</em>.</p>
<table>
<thead>
<tr>
<th align="right">Script Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code>batch_directory (-dir)</code></td>
<td>Directory containing the "batch*.pkl" files. Duplicate lists and batch checkpoints will be saved here.</td>
</tr>
<tr>
<td align="right"><code>process_count (-procs)</code></td>
<td>Number of processes in the pool. Defaults to 4.</td>
</tr>
</tbody>
</table>
<p>The script generates a list of detected duplicates for files/documents located in the various "batch*.pkl" files.</p>
<p>We save file and document level checkpoints after each query to allow for easy resuming.
Each batch file will have a corresponding "*duplicates.txt" when done.</p>
<p>For example on Linux with the default 4 processes:</p>
<pre><code class="language-bash">python -m cleaning.minhash_lsh_dedupe -dir /mnt/data/openwebtext2/scrapes
</code></pre>
<h3 id="de-duplicating-using-generated-duplicate-lists">De-Duplicating Using Generated Duplicate Lists</h3>
<p>This step is performed by <em>cleaning/dedupe_from_indexes.py</em>.</p>
<table>
<thead>
<tr>
<th align="right">Script Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code>batch_directory (-dir)</code></td>
<td>Directory containing the "*duplicates.txt" files along with the "file_name_lookup.pkl" created during batch slicing. The "*final.jsonl.zst" files will be output in their original directories.</td>
</tr>
</tbody>
</table>
<p>This script builds a list of all duplicates by file_id &amp; document_id, and then iterates
through all ".minscored" files from the filename lookup, creating a new archive for each 
file in the original containing all documents that were not marked as duplicates during 
the previous step.</p>
<p>For each original file, a "*final.jsonl.zst" files will be output in the same directory.</p>
<p>For example on Linux:</p>
<pre><code class="language-bash">python -m cleaning.dedupe_from_indexes -dir /mnt/data/openwebtext2/scrapes
</code></pre>
<h2 id="stage-5-packaging-the-dataset-releases">Stage 5 - Packaging The Dataset Releases</h2>
<h3 id="plug-and-play-release">Plug And Play Release</h3>
<p>We originally did processing by month, but now just use files containing scrapes for the original
URL files.</p>
<p>Simply tar all the "*final.jsonl.zst" files.</p>
<h3 id="raw-scrapes-release">Raw Scrapes Release</h3>
<p>Similarly just tar all the "scrapes_*.jsonl.zst" files.</p>
<h2 id="stage-6-produce-release-stats">Stage 6 - Produce Release Stats</h2>
<p>If you move the files from each release into their own subdirectory, you can run the "data_analysis/final_stats.py"
to get total document count and text size for all "jsonl.zst" files in each directory:</p>
<p>For example on Linux:</p>
<pre><code class="language-bash">python -m data_analysis.final_stats -dir /mnt/data/openwebtext2/final
python -m data_analysis.final_stats -dir /mnt/data/openwebtext2/raw_release
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../background/" class="btn btn-neutral" title="WebText Background"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../background/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
